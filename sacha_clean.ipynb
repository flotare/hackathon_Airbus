{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9120d7d3",
   "metadata": {},
   "source": [
    "# Pipeline entraînement + SHAP + prédiction (LightGBM + MCA batch)\n",
    "Notebook nettoyé et reproductible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489125f3",
   "metadata": {},
   "source": [
    "version ordinateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfd791a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\sacha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sacha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (25.0)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Downloading sqlalchemy-2.0.45-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sacha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\sacha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in c:\\users\\sacha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Downloading greenlet-3.3.0-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sacha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\sacha\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "Downloading alembic-1.17.2-py3-none-any.whl (248 kB)\n",
      "Downloading sqlalchemy-2.0.45-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 23.5 MB/s  0:00:00\n",
      "Downloading greenlet-3.3.0-cp313-cp313-win_amd64.whl (301 kB)\n",
      "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: Mako, greenlet, colorlog, sqlalchemy, alembic, optuna\n",
      "\n",
      "   ---------------------------------------- 0/6 [Mako]\n",
      "   ---------------------------------------- 0/6 [Mako]\n",
      "   ---------------------------------------- 0/6 [Mako]\n",
      "   ---------------------------------------- 0/6 [Mako]\n",
      "   ------ --------------------------------- 1/6 [greenlet]\n",
      "   ------ --------------------------------- 1/6 [greenlet]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------- ------------------- 3/6 [sqlalchemy]\n",
      "   -------------------------- ------------- 4/6 [alembic]\n",
      "   -------------------------- ------------- 4/6 [alembic]\n",
      "   -------------------------- ------------- 4/6 [alembic]\n",
      "   -------------------------- ------------- 4/6 [alembic]\n",
      "   -------------------------- ------------- 4/6 [alembic]\n",
      "   -------------------------- ------------- 4/6 [alembic]\n",
      "   -------------------------- ------------- 4/6 [alembic]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   --------------------------------- ------ 5/6 [optuna]\n",
      "   ---------------------------------------- 6/6 [optuna]\n",
      "\n",
      "Successfully installed Mako-1.3.10 alembic-1.17.2 colorlog-6.10.1 greenlet-3.3.0 optuna-4.6.0 sqlalchemy-2.0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script mako-render.exe is installed in 'c:\\Users\\sacha\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script alembic.exe is installed in 'c:\\Users\\sacha\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script optuna.exe is installed in 'c:\\Users\\sacha\\AppData\\Local\\Programs\\Python\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install optuna\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import prince\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import prince\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# -------------------------\n",
    "# PARAMS\n",
    "# -------------------------\n",
    "TRAIN_PATH = \"data/train.csv\"\n",
    "TEST_PATH  = \"data/test.csv\"\n",
    "OUT_PATH   = \"submission.csv\"\n",
    "\n",
    "ID_COL = \"id\"\n",
    "TARGET_COLS = [\"wip\", \"investissement\", \"satisfaction\"]\n",
    "\n",
    "N_PARAM_COLS = 51\n",
    "DROP_ID_COL_IN_X = True\n",
    "\n",
    "SEED = 42\n",
    "K_MCA = 20\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "EARLY_STOPPING_ROUNDS = 200\n",
    "N_TRIALS = 40   # 30–60 est un bon range\n",
    "\n",
    "# Base params (GPU)\n",
    "LGB_BASE = dict(\n",
    "    objective=\"regression\",\n",
    "    metric=\"mae\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    n_estimators=20000,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    verbosity=-1,\n",
    "    device_type=\"gpu\",\n",
    "    gpu_use_dp=False,\n",
    "    max_bin=255,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load + split X/Y\n",
    "# -------------------------\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "assert ID_COL in train_df.columns and ID_COL in test_df.columns\n",
    "for t in TARGET_COLS:\n",
    "    assert t in train_df.columns\n",
    "\n",
    "X_train = train_df.drop(columns=TARGET_COLS)\n",
    "Y_train = train_df[TARGET_COLS].copy()\n",
    "\n",
    "test_ids = test_df[ID_COL].values\n",
    "X_test = test_df.copy()\n",
    "\n",
    "if DROP_ID_COL_IN_X:\n",
    "    X_train = X_train.drop(columns=[ID_COL])\n",
    "    X_test  = X_test.drop(columns=[ID_COL])\n",
    "\n",
    "# -------------------------\n",
    "# 2) demand / param\n",
    "# -------------------------\n",
    "if N_PARAM_COLS == 0:\n",
    "    X_train_demand = X_train.copy()\n",
    "    X_train_param  = pd.DataFrame(index=X_train.index)\n",
    "    X_test_demand  = X_test.copy()\n",
    "    X_test_param   = pd.DataFrame(index=X_test.index)\n",
    "else:\n",
    "    X_train_demand = X_train.iloc[:, :-N_PARAM_COLS].copy()\n",
    "    X_train_param  = X_train.iloc[:, -N_PARAM_COLS:].copy()\n",
    "    X_test_demand  = X_test.iloc[:, :-N_PARAM_COLS].copy()\n",
    "    X_test_param   = X_test.iloc[:, -N_PARAM_COLS:].copy()\n",
    "\n",
    "# -------------------------\n",
    "# 3) train/val split\n",
    "# -------------------------\n",
    "Xd_tr, Xd_va, Xp_tr, Xp_va, Y_tr, Y_va = train_test_split(\n",
    "    X_train_demand, X_train_param, Y_train,\n",
    "    test_size=TEST_SIZE, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 4) MCA fit + transform\n",
    "# -------------------------\n",
    "print(f\"[MCA] Fit sur {X_train_demand.shape}\", flush=True)\n",
    "mca = prince.MCA(n_components=K_MCA, n_iter=3, random_state=SEED).fit(X_train_demand)\n",
    "\n",
    "def mca_transform(df):\n",
    "    Z = mca.transform(df)\n",
    "    Z = pd.DataFrame(Z, index=df.index)\n",
    "    Z.columns = [f\"MCA_{i+1}\" for i in range(Z.shape[1])]\n",
    "    return Z\n",
    "\n",
    "Xmca_tr = mca_transform(Xd_tr)\n",
    "Xmca_va = mca_transform(Xd_va)\n",
    "Xmca_te = mca_transform(X_test_demand)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Build X_final\n",
    "# -------------------------\n",
    "X_final_tr = pd.concat([Xd_tr.reset_index(drop=True), Xmca_tr.reset_index(drop=True), Xp_tr.reset_index(drop=True)], axis=1)\n",
    "X_final_va = pd.concat([Xd_va.reset_index(drop=True), Xmca_va.reset_index(drop=True), Xp_va.reset_index(drop=True)], axis=1)\n",
    "X_final_te = pd.concat([X_test_demand.reset_index(drop=True), Xmca_te.reset_index(drop=True), X_test_param.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# dédup noms + align\n",
    "X_final_tr = X_final_tr.loc[:, ~X_final_tr.columns.duplicated()].copy()\n",
    "X_final_va = X_final_va.loc[:, ~X_final_va.columns.duplicated()].copy()\n",
    "X_final_te = X_final_te.loc[:, ~X_final_te.columns.duplicated()].copy()\n",
    "\n",
    "X_final_va = X_final_va.reindex(columns=X_final_tr.columns, fill_value=0)\n",
    "X_final_te = X_final_te.reindex(columns=X_final_tr.columns, fill_value=0)\n",
    "\n",
    "print(\"Shapes:\", X_final_tr.shape, X_final_va.shape, X_final_te.shape, flush=True)\n",
    "\n",
    "# -------------------------\n",
    "# 6) Optuna tuning per target\n",
    "# -------------------------\n",
    "def tune_one_target(Xtr, ytr, Xva, yva, n_trials=40):\n",
    "    def objective(trial):\n",
    "        params = dict(LGB_BASE)\n",
    "        params.update({\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.10, log=True),\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255),\n",
    "            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 200),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 0, 10),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 0.0, 5.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 0.0, 10.0),\n",
    "        })\n",
    "\n",
    "        m = lgb.LGBMRegressor(**params)\n",
    "        m.fit(\n",
    "            Xtr, ytr,\n",
    "            eval_set=[(Xva, yva)],\n",
    "            eval_metric=\"mae\",\n",
    "            callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=False)]\n",
    "        )\n",
    "        best_iter = int(m.best_iteration_ or params[\"n_estimators\"])\n",
    "        pred = m.predict(Xva, num_iteration=best_iter)\n",
    "        return mean_absolute_error(yva, pred)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    return study.best_params, study.best_value\n",
    "\n",
    "pred_test = {}\n",
    "report = []\n",
    "\n",
    "for t in TARGET_COLS:\n",
    "    print(f\"\\n=== Tuning target: {t} ===\", flush=True)\n",
    "    ytr = Y_tr[t].values\n",
    "    yva = Y_va[t].values\n",
    "\n",
    "    best_params, best_mae = tune_one_target(X_final_tr, ytr, X_final_va, yva, n_trials=N_TRIALS)\n",
    "    print(f\"[BEST] MAE={best_mae:.6f} | params={best_params}\", flush=True)\n",
    "\n",
    "    # Refit final sur (train+val) avec ces params + n_estimators fixé par early stopping\n",
    "    # -> on refait un fit avec early stopping pour récupérer un best_iter cohérent avec les best_params\n",
    "    tmp = lgb.LGBMRegressor(**{**LGB_BASE, **best_params})\n",
    "    tmp.fit(\n",
    "        X_final_tr, ytr,\n",
    "        eval_set=[(X_final_va, yva)],\n",
    "        eval_metric=\"mae\",\n",
    "        callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS, verbose=False)]\n",
    "    )\n",
    "    best_iter = int(tmp.best_iteration_ or LGB_BASE[\"n_estimators\"])\n",
    "    print(f\"[REFIT] best_iter={best_iter}\", flush=True)\n",
    "\n",
    "    X_all = pd.concat([X_final_tr, X_final_va], axis=0).reset_index(drop=True)\n",
    "    y_all = np.concatenate([ytr, yva], axis=0)\n",
    "\n",
    "    final = lgb.LGBMRegressor(**{**LGB_BASE, **best_params, \"n_estimators\": best_iter})\n",
    "    final.fit(X_all, y_all)\n",
    "\n",
    "    pred_test[t] = final.predict(X_final_te)\n",
    "    report.append((t, best_mae, best_iter))\n",
    "\n",
    "# -------------------------\n",
    "# 7) submission.csv\n",
    "# -------------------------\n",
    "sub = pd.DataFrame({ID_COL: test_ids})\n",
    "for t in TARGET_COLS:\n",
    "    sub[t] = pred_test[t]\n",
    "sub.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "print(\"\\nSaved:\", OUT_PATH, flush=True)\n",
    "print(\"\\nRésumé (val Optuna):\", flush=True)\n",
    "for t, mae, best_iter in report:\n",
    "    print(f\"- {t:15s} | MAE={mae:.6f} | best_iter={best_iter}\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
